# LLM API Playground

複数のLLMプロバイダー（Anthropic, OpenAI, Google）のAIモデルとの対話を、統一されたインターフェースで実現する高機能なWebアプリケーションです。ファイルアップロード、モデルごとの動的なパラメータ設定、利用料金の追跡と上限設定など、実践的な機能を備えています。

## ✨ 機能概要

### 対応AIモデル

現在、以下のモデルをサポートしています。モデルの追加や設定は設定ファイルで容易に管理できます。

- **Anthropic Claude シリーズ:** Claude Sonnet 4
- **OpenAI GPT シリーズ:** GPT-4.1, GPT-4.1-mini, GPT-4.1-nano, O3, O4-mini
- **Google Gemini シリーズ:** Gemini 2.5 Pro, Gemini 2.5 Flash

### 主要機能

- **洗練されたUI/UX:**
  - **Markdown対応:** AIからの応答に含まれるMarkdownをリアルタイムで整形し、コードブロック、リスト、見出しなどを美しく表示します。
  - **入力欄の自動可変:** プロンプト入力欄は、入力内容に応じて高さが自動で調整されます。
  - **入力欄の固定表示:** チャット履歴をスクロールしても、プロンプト入力欄は常に画面下部に固定され、いつでもシームレスに入力が可能です。
  - **回答生成の中断:** AIが長い回答を生成している最中でも、ボタン一つで生成を即座に停止できます。

- **マルチプロバイダー対応:**
  - 複数のLLMをUIからシームレスに切り替えて利用可能。

- **動的なモデル設定:**
  - **通常モデル:** 「回答のスタイル（堅実/標準/創造的）」と「最大トークン数」を動的に設定可能。最大トークン数の上限は、各モデルの仕様に合わせて自動で調整されます。
  - **リーゾニングモデル:** 「リーゾニング精度（Low/Middle/High）」をプリセットから選択可能。

- **Web検索連携:**
  - `o3` モデルでは、リアルタイムのWeb検索結果を基に回答を生成する機能が利用可能です。機密情報を含まない公開情報の検索・要約に活用できます。
  - **文脈理解力の強化:** Web検索利用時でも、過去の会話履歴全体を完全に参照します。これにより、「日本の総理大臣は？」→「その人の出身地は？」といった、文脈に依存する質問にも正確に応答できます。

- **ファイルアップロードとテキスト抽出:**
  - 様々な形式のファイルをアップロードし、その内容をプロンプトに含めることができます。
  - **対応ファイル:** 
  PDF, PNG, Word (.docx), Excel (.xlsx), テキスト (.txt), JSON (.json)
  - **処理方法:** 
  Google Cloud Document AI (OCR) や各種ライブラリを用いて、ファイル内容をテキストに変換します。

- **利用料金トラッキングと上限設定:**
  - モデルごとのAPI利用料���をトークン数に基づいて自動で計算し、Firestoreに記録します。
  - 特定のモデルに対して月間の利用料金上限を設定できます。
  - **UIへの警告表示:** モデル選択時、利用料金が上限の8割に達している場合は警告を表示し、上限に達したモデルは自動的に選択不可になります。

- **アップデート情報の確認:**
  - ヘッダーの「アップデート情報」ボタンから、いつでも最新の機能追加や修正に関するリリースノートを確認できます。

- **IPアドレスによるアクセス制限:**
  - 環境変数で指定されたIPアドレスからのアクセスのみを許可する、堅牢なセキュリティ機能を備えています。

## ⚙️ 設定と更新履歴の管理

### モデル設定のカスタマイズ

モデルの定義、料金、利用上限額などの設定は、すべて `src/config/models.json` ファイルで一元管理されています。新しいモデルの追加や既存モデルの設定変更は、このファイルを編集するだけで完了します。

### 更新履歴（リリースノート）の管理

アプリケーションの更新履歴は、プロジェクトのルートディレクトリにある `RELEASE_NOTES.md` ファイルで管理されています。機能追加やバグ修正を行った際は、このファイルにMarkdown形式で変更点を追記してください。記述された内容は、ヘッダーの「アップデート情報」から閲覧できます。

#### 追記フォーマットの例

```markdown
## YYYY-MM-DD

### ✨ 新機能

- 新しい機能の概要を記述します。

### 🐛 バグ修正

- 修正したバグの内容を記述します。
```

### `models.json` の構造

このJSONファイルは、4つの主要なキーで構成されています。

```json
{
  "modelGroups": [],
  "modelConfig": {},
  "monthlyLimitsUSD": {},
  "pricingPerMillionTokensUSD": {}
}
```

#### 1. `modelGroups`

UIのモデル選択ドロップダウンに表示されるモデルのリストとグループを定義します。

- `label`: プロバイダー名など、グループの見出しとして表示され���文字列。
- `models`: グループに所属するモデルを定義するオブジェクト。
  - **キー (例: `"GPT-4.1"`)**: UIに表示されるモデル名。
  - **値 (例: `"gpt-4.1"`)**: アプリケーション内部で使われる一意のモデルID。

#### 2. `modelConfig`

各モデルの具体的な挙動と設定を定義します。キーには `modelGroups` で定義したモデルIDを使用します。

- `type`: モデルの種別を `"normal"` または `"reasoning"` で指定します。これにより、UIに表示される設定項目が切り替わります。
- `maxTokens`: そのモデルが受け付ける最大のトークン数。UIのスライダーの最大値として使用されます。

#### 3. `monthlyLimitsUSD`

特定のモデルに対する月間の利用料金（USD）の上限を設定します。ここにモデルIDと上限額を記述すると、自動で利用状況が監視されます。

- **キー**: 上限を設定したいモデルのID。
- **値**: 上限とする料金（数値）。

#### 4. `pricingPerMillionTokensUSD`

各モデルの100万トークンあたりの料金を定義します。従量課金の計算に利用されます。

- **キー**: 料金を設定するモデルのID。
- **値**: `input` (入力トークン単価) と `output` (出力トークン単価) を持つオブジェクト。

### 新しいモデルの追加手順

例として、新しい `GPT-X` というモデルを追加する場合の手順を以下に示します。

1.  **`modelGroups` に追加:**
    `"OpenAI"` グループに、表示名 `"GPT-X"` とモデルID `"gpt-x"` を追加します。
    ```json
    {
      "label": "OpenAI",
      "models": {
        "GPT-4.1": "gpt-4.1",
        // ... 既存のモデル ...
        "GPT-X": "gpt-x" // <-- ここに追加
      }
    }
    ```

2.  **`modelConfig` に追加:**
    モデルID `"gpt-x"` の設定（種別と最大トークン数）を定義します。
    ```json
    "modelConfig": {
      // ... 既存のモデル設定 ...
      "gpt-x": { "type": "normal", "maxTokens": 128000 } // <-- ここに追加
    }
    ```

3.  **`pricingPerMillionTokensUSD` に追加:**
    モデルID `"gpt-x"` の料金を定義します。
    ```json
    "pricingPerMillionTokensUSD": {
      // ... 既存のモデル料金 ...
      "gpt-x": { "input": 10, "output": 30 } // <-- ここに追加
    }
    ```

4.  **(任意) `monthlyLimitsUSD` に追加:**
    もしこのモデルに利用上限を設ける場合は、IDと上限額を追記します。

以上の4ステップで、コードを一切��更することなく、新しいモデルをアプリケーションに統合できます。

## ☁️ デプロイ

このアプリケーションは、Google Cloud Build を使ってCloud Runにデプロイされるように構成されています。リポジトリへのプッシュをトリガーとして、`cloudbuild.yaml` に定義されたパイプラインが自動的に実行されます。

1.  **Dockerイメージのビルド:** `Dockerfile` を基に、本番用のコンテナイメージがビルドされます。
2.  **Artifact Registryへのプッシュ:** ビルドされたイメージがArtifact Registryに保存されます。
3.  **Cloud Runへのデプロイ:** 最新のイメージを使って、Cloud Runサービスが更新されます。この際、Secret Managerから本番用の環境変数が安全にサービスに渡されます。